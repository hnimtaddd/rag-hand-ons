{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Q&A agent using Langchain\n",
    "\n",
    "\n",
    "In this exercise, we will implement a Q&A agent from scarch using Langchain framework\n",
    "Knowledge of this agent is from t6, all knowledge will be passed to the agent via prompt\n",
    "\n",
    "After complete this hand-ons, we will have some understanding of how to build a simple Q&A agent using Langchain, advandtages and limitations of this approach\n",
    "\n",
    "![rag.png](./public/rag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# these variables are required to initialize Langchain AzureChatOpenAI instance\n",
    "required_env_vars = [\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_API_VERSION\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"AZURE_OPENAI_MODEL\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT_NAME\",\n",
    "]\n",
    "\n",
    "for var in required_env_vars:\n",
    "    if os.environ.get(var) is None:\n",
    "        raise Exception(f\"Missing `{var}` environment variable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2023-03-15-preview\")\n",
    "azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"https://public-api.grabgpt.managed.catwalk-k8s.stg-myteksi.com\")\n",
    "deployment_name=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4-turbo\")\n",
    "model=os.environ.get(\"AZURE_OPENAI_MODEL\", \"gpt-4-turbo\")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    deployment_name=deployment_name,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone git@gitlab.myteksi.net:sentry/t6/t6.git ./tmp/t6 && mkdir -p knowledge/t6 && rsync -avm --include='*.rst' --remove-source-files -f 'hide,! */' \"tmp/t6/doc\" \"knowledge/t6\" && rm -rf tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "BASE_PATH = \"./knowledge/t6/doc/grabdocs\"\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    raise ValueError(f\"Directory {BASE_PATH} does not exist\")\n",
    "\n",
    "loader = DirectoryLoader(path=BASE_PATH, loader_cls=TextLoader, glob=\"**/*.rst\", exclude=[\"index.rst\"])\n",
    "documents: List[Document] = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n\".join([doc.metadata['source'] for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents and len(documents) and print(documents[0].page_content, end=\"\\n<========== END \\n\")\n",
    "# documents and len(documents) and print(documents[1].page_content, end=\"\\n<========== END \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "ADVIRSOR_PROMPT = \"\"\"\n",
    "You are a helpful advisor, collaborating with other agents. \\\n",
    "Don't assume anything you don't know. Use the context below to answer the user question\\\n",
    "Think carefully about the question and provide the best answer you can. \\\n",
    "If you are unable to fully answer, that's OK, another agent with different tools will help where you left off. \\\n",
    "If you or any of the other agents have the final answer or deliverable, \\\n",
    "prefix your respond with FINAL ANSWER so the team knows to stop.\n",
    "The context is: {context}.\\n\n",
    "\"\"\"\n",
    "context = \"\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "chain_prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\", ADVIRSOR_PROMPT),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# In overview, this chain will receive a dictionary with the key \"input\" and \"context\", and return the dictionary with the key \"answer\"\n",
    "naive_chain = RunnableSequence(\n",
    "    chain_prompt,\n",
    "    llm.with_config(run_name=\"llm\"),\n",
    "    {\"answer\": StrOutputParser().with_config(run_name=\"parser\")},\n",
    ").with_config(run_name=\"naive_chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chain_prompt.invoke({\"input\": \"\", \"context\":context }).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query =(\n",
    "    \"I have error `Fail to push image` while running cop_image:envoy-base step in \"\n",
    "    \"pre stage while setting up t6 fabric pipeline, how to resolve it?\"\n",
    ")\n",
    "from openai import RateLimitError\n",
    "\n",
    "try:\n",
    "    output = naive_chain.invoke({\"input\": user_query, \"context\":context })\n",
    "    print(output[\"answer\"])\n",
    "\n",
    "except RateLimitError as e:\n",
    "    print(\"Exceed rate limit\")\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that the answer is not correct, we have some main reasons for this:\n",
    "- AI under the hood is a black box, we don't know how it works, so with the context of the question which is very large, we couldn't sure that the model already catch the right context of the question (context-window limit)\n",
    "- Hallucination of the model, the model could generate an answer that looks genuine but actually not\n",
    "\n",
    "Now, try to use this naive llm with a longger context as in real world scenario, and see how it performs \n",
    "\n",
    "For the example scenario, we will double the context length to pass to the chain_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chain_prompt.invoke({\"input\": \"\", \"context\":context + context }).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import RateLimitError\n",
    "\n",
    "user_query = (\n",
    "    \"I have error `Fail to push image` while running cop_image:envoy-base step in \"\n",
    "    \"pre stage while setting up t6 fabric pipeline, how to resolve it?\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    output = naive_chain.invoke({\"input\": user_query, \"context\": context + context})\n",
    "\n",
    "    print(output[\"answer\"])\n",
    "except RateLimitError as e:\n",
    "    print(\"Exceed rate limit\")\n",
    "\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to workaround this issue?\n",
    "\n",
    "We will try to narrow down the context to the most relevant information to the question,\n",
    "\n",
    "In this case, we will try to add the `reference` field that specifies the path of the file that may contain the relevant information to the question\n",
    "\n",
    "It is a naive RAG model ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# we now have 1 more attribute `reference` in the input, then this chain will read the content of the file and pass it to the next chain as `context`\n",
    "naive_retriever_chain = RunnableSequence(\n",
    "    RunnablePassthrough.assign(\n",
    "        context=RunnableSequence(\n",
    "            (lambda x: TextLoader(file_path=x[\"reference\"]).load()),\n",
    "            (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "        )\n",
    "    ),\n",
    "    naive_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = (\n",
    "    \"I have error `Fail to push image` while running cop_image:envoy-base step in \"\n",
    "    \"pre stage while setting up t6 fabric pipeline, how to resolve it?\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    output = naive_retriever_chain.invoke(\n",
    "        {\n",
    "            \"input\": user_query,\n",
    "            \"reference\": \"./knowledge/t6/doc/grabdocs/automation/gitlab_ci_automation.rst\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(output)\n",
    "except RateLimitError as e:\n",
    "    print(\"Exceed rate limit\")\n",
    "\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is quite good now (at least, it's answer the question correctly), but we spend more effort to narrow down the context (human retrival ðŸ™‚), and it's not always the case that we can narrow down the context to the most relevant information\n",
    "\n",
    "We could see that this approach is not scalable, and it's not efficient in real-world scenarios, especially when the knowledge base is very large\n",
    "\n",
    "\n",
    "In summary, the naive Q&A agent have some good points:\n",
    "- It's easy to implement\n",
    "- Sufficent for simple Q&A tasks\n",
    "- With small knowledge base, it could perform well\n",
    "\n",
    "But it has some limitations:\n",
    "- Not scalable\n",
    "- Not efficient in real-world scenarios\n",
    "- Can't handle large knowledge base\n",
    "- Token limit\n",
    "\n",
    "In the next notebook, we will try to use the another `retrieval` approach to get the most relevant information to the question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
