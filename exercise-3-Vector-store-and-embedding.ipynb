{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document embedding and Vector store\n",
    "\n",
    "There are many ways to build a retriever in RAG model, one of the most common ways is to use a document embedding and a vector store.\n",
    "\n",
    "The document embedding is a fixed-size representation of the document, and the vector store is a database that stores the embeddings of all the documents.\n",
    "\n",
    "When we want to retrieve documents, we can calculate the similarity between the query embedding and the document embeddings in the vector store, and return the documents with the highest similarity scores, these documents are expected to be relevant to the query.\n",
    "\n",
    "![rag.png](./public/rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loading\n",
    "\n",
    "Document often stored in diferent format (such as json, csv, txt, ...) and different places (such as local file, internet, database, ...).\n",
    "\n",
    "Document loading is the technique to load the document from different sources and convert them into a unified format.\n",
    "\n",
    "In this example, we assume the we already have all the documents stored in `./knowledge` folder, and each document file is a rst file.\n",
    "\n",
    "If you want to pull the documents, run this command:\n",
    "```bash \n",
    "    git clone git@gitlab.myteksi.net:sentry/t6/t6.git ./tmp/t6\n",
    "\n",
    "    mkdir -p knowledge/t6\n",
    "\n",
    "    rsync -avm --include='*.rst' --remove-source-files -f 'hide,! */' \"tmp/t6/doc\" \"knowledge/t6\" \n",
    "\n",
    "    rm -rf tmp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone git@gitlab.myteksi.net:sentry/t6/t6.git ./tmp/t6 && mkdir -p knowledge/t6 && rsync -avm --include='*.rst' --remove-source-files -f 'hide,! */' \"tmp/t6/doc\" \"knowledge/t6\" && rm -rf tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 50,\n",
    "    chunk_overlap = 10,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "example_string = (\n",
    "    \"This is a really long string and I want to see how it works with text splitter.\"\n",
    "    \"This is a really long string and I want to see how it works with text splitter.\"\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_text(example_string)\n",
    "\n",
    "print(len(example_string))\n",
    "print(len(splits))\n",
    "print(splits[0])\n",
    "print(splits[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above example, we could use splitter to split the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "BASE_PATH = \"./knowledge/t6\"\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    raise ValueError(f\"Directory {BASE_PATH} does not exist\")\n",
    "\n",
    "# prepare the documents\n",
    "loader = DirectoryLoader(\n",
    "    path=BASE_PATH, loader_cls=TextLoader, glob=\"**/*.rst\", exclude=[\"index.rst\"]\n",
    ")\n",
    "documents: List[Document] = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\"],\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(splits))\n",
    "\n",
    "print(splits[0], end=\"\\n<========================>\\n\\n\")\n",
    "print(splits[1], end=\"\\n<========================>\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading document, we could have a list of documents, we then could we these document to answer the question.\n",
    "\n",
    "As we've dicussed before, we need to embedding these documents and store them in a vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document embedding\n",
    "\n",
    "Document embedding is the technique to convert the document into a fixed-size representation a.k.a embedding vector.\n",
    "\n",
    "There are many embedding techniques, but in the context of this hand-ons, we assume that embedding here is semantic embedding.\n",
    "\n",
    "Under the hood, the document embedding is often implemented by an AI model that have ability to **catch the semantic meaning of text** and **encode them into a fixed-size vector**.\n",
    "\n",
    "So in general, we expect that if 2 documents are **similar or have similar meaning**, their **embeddings should be close to each other in the vector space**.\n",
    "\n",
    "In this example, we will use an API from Azure to get the embedding of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# these variables are required to initialize Langchain AzureChatOpenAI instance\n",
    "required_env_vars = [\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_API_VERSION\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"AZURE_OPENAI_MODEL\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT_NAME\",\n",
    "    \"AZURE_OPENAI_EMBEDDING_MODEL\",\n",
    "    \"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\",\n",
    "]\n",
    "\n",
    "for var in required_env_vars:\n",
    "    if os.environ.get(var) is None:\n",
    "        raise Exception(f\"Missing `{var}` environment variable\")\n",
    "\n",
    "\n",
    "api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2023-03-15-preview\")\n",
    "azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"https://public-api.grabgpt.managed.catwalk-k8s.stg-myteksi.com\")\n",
    "deployment_name=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4-turbo\")\n",
    "model=os.environ.get(\"AZURE_OPENAI_MODEL\", \"gpt-4-turbo\")\n",
    "embedding_model=os.environ.get(\"AZURE_OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-large\")\n",
    "embedding_deployment_name=os.environ.get(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\", \"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    azure_deployment=embedding_deployment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's make some experiment to see how embedding works with Azure API.\n",
    "\n",
    "We will embedding different text with different sematic meaning and see how the embedding vectors look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"Today is pleasantly warm with a gentle breeze.\"\n",
    "sentence2 = \"The temperature today is mild and the air is calm.\"\n",
    "sentence3 = \"Itâ€™s uncomfortably hot and humid today.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.dot(embedding1, embedding2))\n",
    "print(np.dot(embedding2, embedding3))\n",
    "print(np.dot(embedding3, embedding1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, it's easy to see that the sentence1 and sentence2 have similar meaning, and theirfor their embeddings are closer compared to the sentence3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf ./docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = './docs/chroma/'\n",
    "\n",
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=splits,\n",
    "#     embedding=embedding,\n",
    "#     persist_directory=persist_directory\n",
    "# )\n",
    "\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\" how to resolve Fail to push image error while running cop_image:envoy-base stage in ci/cd pipeline?\"\n",
    "candidates = vectordb.similarity_search(question, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for candidate in candidates:\n",
    "    print(candidate.metadata)\n",
    "    print(candidate.page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that all retrieved documents have similar content with the query, which means that we already gat sufficient information that are ready to feed into the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
