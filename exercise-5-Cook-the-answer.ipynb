{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies setup\n",
    "\n",
    "We will setup the retrieval_chain and an LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# these variables are required to initialize Langchain AzureChatOpenAI instance\n",
    "required_env_vars = [\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_API_VERSION\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"AZURE_OPENAI_MODEL\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT_NAME\",\n",
    "    \"AZURE_OPENAI_EMBEDDING_MODEL\",\n",
    "    \"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\",\n",
    "]\n",
    "\n",
    "for var in required_env_vars:\n",
    "    if os.environ.get(var) is None:\n",
    "        raise Exception(f\"Missing `{var}` environment variable\")\n",
    "\n",
    "\n",
    "api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2023-03-15-preview\")\n",
    "azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"https://public-api.grabgpt.managed.catwalk-k8s.stg-myteksi.com\")\n",
    "deployment_name=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4-turbo\")\n",
    "model=os.environ.get(\"AZURE_OPENAI_MODEL\", \"gpt-4-turbo\")\n",
    "embedding_model=os.environ.get(\"AZURE_OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-large\")\n",
    "embedding_deployment_name=os.environ.get(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\", \"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "class LoggingHandler(BaseCallbackHandler):\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs\n",
    "    ) -> None:\n",
    "        print(f\"Chain {serialized.get('name')} started\")\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:\n",
    "        print(f\"Chain ended, outputs: {outputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import  Markdown, display\n",
    "\n",
    "def beatify_markdown(input: str):\n",
    "    display(Markdown(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    azure_deployment=embedding_deployment_name,\n",
    ")\n",
    "\n",
    "persist_directory = \"./docs/chroma\"\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding,\n",
    ")\n",
    "\n",
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough\n",
    "\n",
    "# Retrieve chain that retrieves the candidates from the key \"input\" and pass to key \"context\"\n",
    "retriever_chain = RunnablePassthrough.assign(\n",
    "    context=RunnableSequence(\n",
    "        (lambda x: x[\"input\"]),\n",
    "        vectordb.as_retriever(),\n",
    "    ),\n",
    ").with_config(run_name=\"retriever\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    azure_deployment=deployment_name,\n",
    ")\n",
    "QA_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful advisor, collaborating with other agents. \\\n",
    "Don't assume anything you don't know. Use the context below to answer the user question\\\n",
    "Think carefully about the question and provide the best answer you can. \\\n",
    "If you are unable to fully answer, that's OK, just leave follow-up questions for the user. \\\n",
    "You must response in the markdown format. \\n\n",
    "\n",
    "context: {context}.\\n\n",
    "input: {input}\n",
    "helpful answer:\n",
    "\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_template(template=QA_SYSTEM_PROMPT)\n",
    "answer_chain = RunnablePassthrough.assign(\n",
    "    answer=RunnableSequence(\n",
    "        qa_prompt,\n",
    "        llm,\n",
    "        StrOutputParser(),\n",
    "    ),\n",
    ").with_config(run_name=\"LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query =(\n",
    "    \"I have error `Fail to push image` while running cop_image:envoy-base step in \"\n",
    "    \"pre stage while setting up t6 fabric pipeline, how to resolve it?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cook the answer with LLM\n",
    "\n",
    "After retrieve the relevant information from the retrievers, we will pass them into the LLM model to generate answer.\n",
    "\n",
    "In this stage, we could use techniques to optimize the context of the question, such as:\n",
    "- Naive query\n",
    "- Map-Reduce \n",
    "- Refine answer\n",
    "- Map-rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive context\n",
    "\n",
    "Finally, after selecting a set of relevant documents for the user query, the next step is to pass all this data into the LLM to generate the answers. A typical strategy is to concatenate all the text from the documents into the prompt.\n",
    "\n",
    "![naive_query](./public/naive_context.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def naive_context(_input: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # we will read relevant document in the field \"context\"\n",
    "    # then concatenate the documents to form the context\n",
    "    # and pass to the field \"context\"\n",
    "    documents: List[Document] = _input.get(\"context\", [])\n",
    "    context = \"\\n\".join([doc.page_content for doc in documents])\n",
    "    return {\n",
    "        **_input,\n",
    "        \"context\": context,\n",
    "    }\n",
    "\n",
    "\n",
    "naive_query_chain = RunnableSequence(\n",
    "    retriever_chain,\n",
    "    naive_context,\n",
    "    answer_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = naive_query_chain.with_config(callbacks=[LoggingHandler()]).invoke({\"input\": user_query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beatify_markdown(answer['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map-Reduce\n",
    "\n",
    "Naive context has a weakness, is that the length of our context can be very large if the number of documents is high. This will cause the context of the LLM to expand and sometimes suffer from context overflow.\n",
    "\n",
    "Map-reduce strategy, instead of passing documents directly to answer the query, is to iterate through the documents to extract information that may be relevant to answering the question.\n",
    "\n",
    "This is particularly useful when we have many documents because we can extract information, synthesize results, and repeat this process until the combined text fits the context length of the LLM.\n",
    "\n",
    "![map_reduce](./public/map_reduce_context.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACT_SYSTEM_PROMPT = \"\"\"\n",
    "You are expert in extracting information from the text.\n",
    "Use the context below to extract the relevant information,\n",
    "these information will be used by other agents to answer the user questions.\n",
    "\n",
    "The more information you can extract the better, but don't extract irrelevant information,\n",
    "If you don't find any information, that's OK, just leave the original content.\n",
    "context: {context}.\n",
    "\n",
    "question: {input}\n",
    "\n",
    "valuable information:\n",
    "\"\"\"\n",
    "\n",
    "extract_prompt = ChatPromptTemplate.from_template(\n",
    "    template=EXTRACT_SYSTEM_PROMPT,\n",
    ")\n",
    "extract_chain = RunnablePassthrough.assign(\n",
    "    context=RunnableSequence(\n",
    "        extract_prompt,\n",
    "        llm,\n",
    "        StrOutputParser(),\n",
    "    ),\n",
    ").with_config(run_name=\"EXTRACTOR\")\n",
    "\n",
    "\n",
    "def reduce_document(inputs: Dict[str, Any]) -> str:\n",
    "    extracted_contexts = inputs.get(\"context\", \"\")\n",
    "    return {\n",
    "        **inputs,\n",
    "        \"context\": \"\\n\".join(extracted_contexts),\n",
    "    }\n",
    "\n",
    "\n",
    "REDUCE_SYSTEM_PROMPT = \"\"\"\n",
    "Combine these summaries: {context}\n",
    "\"\"\"\n",
    "reduce_prompt = ChatPromptTemplate.from_template(\n",
    "    template=REDUCE_SYSTEM_PROMPT,\n",
    ")\n",
    "reduce_chain = RunnablePassthrough.assign(\n",
    "    context = RunnableSequence(\n",
    "        reduce_document,\n",
    "        reduce_prompt,\n",
    "        llm,\n",
    "        StrOutputParser(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "map_reduce_chain = RunnableSequence(\n",
    "    retriever_chain,\n",
    "    extract_chain,\n",
    "    reduce_chain,\n",
    "    answer_chain,\n",
    ").with_config(callbacks=[LoggingHandler()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = map_reduce_chain.invoke({\"input\": user_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beatify_markdown(answer['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Refine answer\n",
    "\n",
    "Refine answer strategy is to gradually refine the answer when iterating through documents and incorporating context.\n",
    "\n",
    "![refine_answer](./public/refine_context.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "\n",
    "INITIAL_SYSTEM_PROMPT = \"\"\"\n",
    "Summarize this context, the summary will be used by other agents to answer the user questions.\n",
    "context: {context}\n",
    "question: {input}\n",
    "\"\"\"\n",
    "summarize_prompt = ChatPromptTemplate.from_template(INITIAL_SYSTEM_PROMPT)\n",
    "initial_llm_chain = RunnableSequence(\n",
    "    summarize_prompt,\n",
    "    llm,\n",
    "    StrOutputParser(),\n",
    ")\n",
    "\n",
    "REFINE_SYSTEM_PROMPT = \"\"\"\n",
    "Given your last context summary, and the updated context, refine your summary.\n",
    "Updated summary will be used by other agents to answer the user questions.\n",
    "Prev summary: {prev_response}\n",
    "Updated context: {context}\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "refine_prompt = ChatPromptTemplate.from_template(REFINE_SYSTEM_PROMPT)\n",
    "refine_llm_chain = RunnableSequence(\n",
    "    refine_prompt,\n",
    "    llm,\n",
    "    StrOutputParser(),\n",
    ")\n",
    "\n",
    "def refine_chain(\n",
    "    initial_llm_chain,\n",
    "    refine_llm_chain,\n",
    ")-> Runnable:\n",
    "    def helper(inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        documents: List[Document] = inputs.get(\"context\", [])\n",
    "        summary = initial_llm_chain.invoke(inputs)\n",
    "        for doc in documents[1:]:\n",
    "            prev_response=summary\n",
    "            summary = refine_llm_chain.invoke(\n",
    "                {**inputs, \"prev_response\": prev_response, \"context\": doc.page_content}\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            **inputs,\n",
    "            \"context\": f\"{inputs['prev_response']}\\n{inputs['context']}\",\n",
    "        }\n",
    "    return helper\n",
    "\n",
    "\n",
    "refine_answer_chain = RunnableSequence(\n",
    "    retriever_chain,\n",
    "    refine_chain(initial_llm_chain, refine_llm_chain),\n",
    "    answer_chain,\n",
    ").with_config(callbacks=[LoggingHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_answer_chain.invoke({\"input\": user_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map-rerank\n",
    "\n",
    "This strategy iterates through the documents and attempts to answer accompanying questions with a score indicating the level of confidence in the answer.\n",
    "\n",
    "These answers then will be reranked based on the score, the highest score will be selected as the final answer.\n",
    "\n",
    "![map_rerank](./public/map_rerank.webp)\n",
    "\n",
    "Note: `ArgMax` is a function to return the argument that give the maximum value of a function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
