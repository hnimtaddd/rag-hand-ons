{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's wrap it up\n",
    "In this hand-ons, we will join all the concepts we've learned so far in order to create a Q&A agent.\n",
    "\n",
    "Let's create an Q&A pipeline using the folowing steps:\n",
    "1. Load the data\n",
    "2. Preprocess the data: chunk the data into smaller parts\n",
    "3. Prepare knowledge base: embed the data and store in a vector database\n",
    "4. Prepare Q&A chain with three steps:\n",
    "   1. Prepare the question\n",
    "   2. Retrieve the most similar documents\n",
    "   3. Answer the question based on the retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "BASE_PATH = \"./knowledge/t6\"\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    raise ValueError(f\"Directory {BASE_PATH} does not exist\")\n",
    "\n",
    "# prepare the documents\n",
    "loader = DirectoryLoader(\n",
    "    path=BASE_PATH, loader_cls=TextLoader, glob=\"**/*.rst\", exclude=[\"index.rst\"]\n",
    ")\n",
    "documents: List[Document] = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\"],\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# these variables are required to initialize Langchain AzureChatOpenAI instance\n",
    "required_env_vars = [\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_API_VERSION\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"AZURE_OPENAI_MODEL\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT_NAME\",\n",
    "    \"AZURE_OPENAI_EMBEDDING_MODEL\",\n",
    "    \"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\",\n",
    "]\n",
    "\n",
    "for var in required_env_vars:\n",
    "    if os.environ.get(var) is None:\n",
    "        raise Exception(f\"Missing `{var}` environment variable\")\n",
    "\n",
    "\n",
    "api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2023-03-15-preview\")\n",
    "azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"https://public-api.grabgpt.managed.catwalk-k8s.stg-myteksi.com\")\n",
    "deployment_name=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4-turbo\")\n",
    "model=os.environ.get(\"AZURE_OPENAI_MODEL\", \"gpt-4-turbo\")\n",
    "embedding_model=os.environ.get(\"AZURE_OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-large\")\n",
    "embedding_deployment_name=os.environ.get(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\", \"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the knowledge base is empty\n",
    "! rm -rf ./docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    azure_deployment=embedding_deployment_name,\n",
    ")\n",
    "\n",
    "persist_directory = './docs/chroma/'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_core.runnables import (\n",
    "    RunnableSequence,\n",
    "    RunnablePassthrough,\n",
    "    RunnableParallel,\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "ADVIRSOR_PROMPT = \"\"\"\n",
    "You are a helpful advisor, collaborating with other agents. \\\n",
    "Don't assume anything you don't know. Use the context below to answer the user question\\\n",
    "Think carefully about the question and provide the best answer you can. \\\n",
    "If you are unable to fully answer, that's OK, another agent with different tools will help where you left off. \\\n",
    "If you or any of the other agents have the final answer or deliverable, \\\n",
    "prefix your respond with FINAL ANSWER so the team knows to stop.\n",
    "The context is: {context}.\\n\n",
    "\"\"\"\n",
    "\n",
    "context = \"\\n\".join([doc.page_content for doc in documents])\n",
    "chain_prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\", ADVIRSOR_PROMPT),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieve_chain = RunnableSequence(\n",
    "    lambda x: x[\"input\"],\n",
    "    vectordb.as_retriever(),\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs: List[Document]):\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    deployment_name=deployment_name,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "chain = RunnableSequence(\n",
    "    RunnablePassthrough.assign(\n",
    "        candidates=retrieve_chain.with_config(run_name=\"retrieve_chain\"),\n",
    "    ),\n",
    "    RunnableParallel(\n",
    "        answer=RunnableSequence(\n",
    "            RunnablePassthrough.assign(context=lambda x: format_docs(x[\"candidates\"])),\n",
    "            chain_prompt,\n",
    "            llm.with_config(run_name=\"llm\"),\n",
    "            StrOutputParser().with_config(run_name=\"parser\"),\n",
    "        ),\n",
    "        references=lambda docs: [doc.metadata for doc in docs[\"candidates\"]],\n",
    "    ),\n",
    ").with_config(run_name=\"naive_chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query =(\n",
    "    \"I have error `Fail to push image` while running cop_image:envoy-base step in \"\n",
    "    \"pre stage while setting up t6 fabric pipeline, how to resolve it?\"\n",
    ")\n",
    "\n",
    "from openai import RateLimitError\n",
    "\n",
    "try:\n",
    "    output = chain.invoke({\"input\": user_query})\n",
    "    print(output[\"answer\"])\n",
    "    for ref in output[\"references\"]:\n",
    "        print(ref)\n",
    "except RateLimitError as e:\n",
    "    print(\"Exceed rate limit\")\n",
    "    print(str(e))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
